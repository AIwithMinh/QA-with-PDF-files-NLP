# -*- coding: utf-8 -*-
"""Demo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kXUJAlFsaZ1EWuvRG5ivQA6EP7MFW6_U
"""

!pip install -q transformers==4.41.2
!pip install -q bitsandbytes==0.43.1
!pip install -q accelerate==0.31.0
!pip install -q langchain==0.2.5
!pip install -q langchainhub==0.1.20
!pip install -q langchain-chroma==0.1.1
!pip install -q langchain-community==0.2.5
!pip install -q langchain-openai==0.1.9
!pip install -q langchain_huggingface==0.0.3
!pip install -q chainlit==1.1.304
!pip install -q python-dotenv==1.0.1
!pip install -q pypdf==4.2.0
!npm install -g localtunnel
!pip install -q numpy==1.24.4
#Tải

import torch
#Khai báo thư viện
from transformers import BitsAndBytesConfig
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_huggingface.llms import HuggingFacePipeline

from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.chains import ConversationalRetrievalChain

from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub

MODEL_NAME = "lmsys/vicuna-7b-v1.5"
nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
) # Lượng tử hóa

# load mô hình
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=nf4_config,
    low_cpu_mem_usage=True
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

#Cài đặt token
model_pipline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    pad_token_id=tokenizer.eos_token_id,
    device_map="auto"
)

prompt = """
Hello, who are you?
"""
# Few-short learning

device ="cuda" if torch.cuda.is_available() else "cpu"
model_inputs =tokenizer(prompt, return_tensors = "pt").to(device)

generated_ids = model.generate(**model_inputs)[0]
answer = tokenizer.decode(generated_ids, skip_special_tokens=True)
answer.split('\n\n')[1]

prompt = """
### A chat between a human and an assistant.

### Human:
Your task is to classify the sentiment of input text into one of two categories: neutral or negative. Here is an example:

Input: You're great.
Output: Normal

Now, let's practice:
Input: Why are you sad?
Output:
### Assistant:
"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model_inputs = tokenizer(prompt, return_tensors="pt").to(device)

generated_ids = model.generate(**model_inputs)[0]
answer = tokenizer.decode(generated_ids,
                          temperature=1.2,
                          do_sample=True,
                          skip_special_tokens=True)

answer.split('Assistant:\n')[1]

